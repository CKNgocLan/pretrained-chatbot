<link href="style.css" rel="stylesheet"></link>

# Prepprocess Dataset
[How to process data for training](https://huggingface.co/docs/transformers/en/preprocessing)

Process input data into expected model input format.

The data need to be converted and assembled into batches of tensors. For input format of text, use [**Tokenizer**](https://huggingface.co/docs/transformers/en/main_classes/tokenizer) to convert.

The **Tokenizer** will:

* Convert text into a sequence of tokens
* Create a numerical representation of the tokens
* Assemble them into tensors

> #### **_Note:_**
> If you plan on using a pretrained model, it's important to user the associated pretrained tokenizer.

## I. Encode
The [**<method>AutoTokenizer.from_pretrained()</method>**](https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) method is a pretrained tokenizer which downloads the <hyperparam>vocab</hyperparam> a model was pretrained.

To load pretrained tokenizer, invoke <method>*from_pretrained*()</method> method. After invoking, the tokenizer returns a dictionary with 3 items: <result>input_ids</result>, <result>attention_mark</result>, <result>token_type_ids</result>.

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)
```

> #### **_Note_**:
> In case of multiple sentences, just pass as array.

Retrieve passed input by decoding the <result>input_ids</result>:

```python
tokenizer.decode(encoded_input["input_ids"])
```

## II. Pad
The model input must have a uniform shape, though passed sentences usually are not the same length. So **Padding** is a strategy for ensuring tensors are rectangular by <ins>*adding special adding token*</ins> to **shorter sentences**. For example, the special token is **0**.

To pad, set the <hyperparam>padding</hyperparam> parameter to *True*:
```python
encoded_input = tokenizer(batch_sentences, padding=True)
```

## III. Truncation
In case of sentences too long for model to handle, you need to truncate the sentences to shorter length, by setting <hyperparam>truncation</hyperparam> parameter to *True*. The model will truncate sequence to the <span class="highlight">maximum length accepted by the model</span>:

```python
encoded_input = tokenizer(
	batch_sentences
	, padding=True
	, truncation=True
)
```

## IV. Build Tensor
Finally, you want the tokenizer to *return the actual tensors* that get fed to the model.

Set the <hyperparam>return_tensors</hyperparam> parameter to either *pt* for PyTorch, or *tf* for TensorFlow
